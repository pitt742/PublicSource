{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w03OdDnRWwNR"
   },
   "source": [
    "# Image Captioning 과제 \n",
    "\n",
    "### 과제 기한:  08/16\n",
    "### 제출 메일 : cb_park@korea.ac.kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xrnD_uE6fC-"
   },
   "outputs": [],
   "source": [
    "import torch # torch library \n",
    "import torch.nn as nn # Nueral Network에 대한 package\n",
    "import numpy as np  # numpy \n",
    "import torch.nn.functional as F # pytorch function 들을 사용하기 위한 용도 \n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i47erWJTWUnJ"
   },
   "source": [
    "**과제 설명**\n",
    "\n",
    "이번 과제에서는 다음의 Attention을 구현하도록 합니다  (맨 밑의 그림 참조)\n",
    "1. Dot 기반 \n",
    "2. General 기반\n",
    "\n",
    "위의 각각의 Attention을 수식을 보고 구현하면 됩니다.\n",
    "\n",
    "주석으로 * TO DO * 처리 된 영역을 채워주세요 Input과 Return 의 크기는 주석으로 적어 두었습니다. \n",
    "\n",
    "코드 작성후 각 모듈의 Unit test code를 통과 했다면 코드는 올바르게 작동한 것입니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*   AttentionDot 클래스는 Dot 기반의 attention model 입니다.\n",
    "    ( Line by line 으로 주석을 달아놓았으니, 참고해서 작성해주세요. )\n",
    "    \n",
    "    => AttentionDot 클래스가 올바로 작성되었다면, test_attention_dot 함수를 선언한 뒤 호출했을 때,\n",
    "    PASS! Good job! 이라는 메시지가 출력됩니다.\n",
    "\n",
    "\n",
    "*   Attention_General 클래스는 Dot product 이전에 linear layer를 하나 더 통과시킨 뒤  query와 key간의 내적을 하는 모듈입니다.\n",
    "    위와 마찬가지로, 주석을 참고해서 작성해주세요.\n",
    "    (주의: key에 대해서만 새로이 추가된 weight를 적용하고, value에 대해서는 weight를 적용하지 않습니다.)\n",
    "    \n",
    "    => Attention_General 클래스가 올바로 작성되었다면, PASS! Good job! 이라는 메시지가 출력됩니다.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "85JkOE9HHWyI"
   },
   "source": [
    "![대체 텍스트](https://i.stack.imgur.com/tiQkz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOijqRL277pE"
   },
   "source": [
    "**1-1 : Attention  Dot-product Attention**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NB230uBd79_Z"
   },
   "outputs": [],
   "source": [
    "class AttentionDot(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(AttentionDot, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, key, value, query):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param key: encoded features, a tensor of dimension (batch_size, num_pixels, hidden_state)\n",
    "        :param value: encoded features, a tensor of dimension (batch_size, num_pixels, hidden_state)\n",
    "        \n",
    "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, hidden_state)\n",
    "        :return: \n",
    "          attention weighted encoding : (batch_size, hidden_state) - context vector (attention output )\n",
    "          alpha : (B,num_pixels) -> (attention value, which is normalized by softmax function)\n",
    "        \n",
    "        \"\"\"\n",
    "        ## To do (???를 채우세요)\n",
    "        transpose_encoder_out =  key.permute(0,2,1) # (batch_size, num_pixels, hidden_state) -> (batch_size, hidden_state, num_pixels,)\n",
    "        logits = torch.bmm(query.unsqueeze(1), transpose_encoder_out) # ?????? # (batch_size,1, num_pixels)            \n",
    "        logits = logits.squeeze(1) # (batch_size, 1, num_pixels) -> (batch_size, num_pixels)\n",
    "        alpha =  self.softmax(logits) # ??????  # (batch_size, num_pixels)\n",
    "        \n",
    "        attention_weighted_encoding = torch.bmm(alpha.unsqueeze(1), value) #alpha.unsqueeze(1).bmm(value).squeeze(1) # ??????  # (batch_size, hidden_state_dim)\n",
    "        \n",
    "        return attention_weighted_encoding, alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQLicGBBfA-S"
   },
   "source": [
    "테스트 코드 -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTuZ6Uc6XMWo"
   },
   "outputs": [],
   "source": [
    "# 수정하지 말 것 \n",
    "def test_attention_dot():\n",
    "  model = AttentionDot()\n",
    "  test_encoder_out = [\n",
    "      torch.zeros((1,3,2)),\n",
    "      torch.ones((1,3,2)),\n",
    "      torch.zeros((1,3,2)),\n",
    "      torch.FloatTensor([[[1,2],[3,4],[5,6]]]),\n",
    "      torch.FloatTensor([[[6,5],[4,3],[2,1]]]),\n",
    "      torch.FloatTensor([[[1,0],[1,0],[1,0]]]),\n",
    "      torch.FloatTensor([[[0,1],[0,1],[0,1]]]),\n",
    "      torch.FloatTensor([[[1,1],[1,1],[1,1]]])\n",
    "  ]\n",
    "  test_decoder_hidden = [\n",
    "      torch.zeros((1,2)),\n",
    "      torch.ones((1,2)),\n",
    "      torch.ones((1,2)),\n",
    "      torch.FloatTensor([[1,1]]),\n",
    "      torch.FloatTensor([[2,3]]),\n",
    "      torch.FloatTensor([[0,1]]),\n",
    "      torch.FloatTensor([[0,1]]),\n",
    "      torch.FloatTensor([[1,0]])\n",
    "  ]\n",
    "  \n",
    "  test_dot = []\n",
    "#   test_encoder_out = torch.zeros((1,3,10))\n",
    "#   test_decoder_hidden = torch.zeros((1,10))\n",
    "  for i in range(len(test_encoder_out)):\n",
    "    attention_context, alpha = model(test_encoder_out[i], test_encoder_out[i], test_decoder_hidden[i])\n",
    "    test_dot += [alpha]\n",
    "\n",
    "  gt_alpha1 = [\n",
    "      torch.zeros((1,3)).fill_(1/3),\n",
    "      torch.zeros((1,3)).fill_(1/3),\n",
    "      torch.zeros((1,3)).fill_(1/3),\n",
    "      torch.FloatTensor([[3.2932e-04, 1.7980e-02, 9.8169e-01]]),\n",
    "      torch.FloatTensor([[9.9995e-01, 4.5398e-05, 2.0611e-09]]),\n",
    "      torch.zeros((1,3)).fill_(1/3),\n",
    "      torch.zeros((1,3)).fill_(1/3),\n",
    "      torch.zeros((1,3)).fill_(1/3)\n",
    "  ]\n",
    "\n",
    "  result = [torch.all(torch.lt(torch.abs(torch.add(test_dot[i], -gt_alpha1[i])), 1e-4)).item() for i in range(len(test_encoder_out))]\n",
    "\n",
    "  \n",
    "  assert (torch.FloatTensor(result).sum().item() == len(test_encoder_out)) == 1\n",
    "  \n",
    "  print (\"PASS! Good job! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WsKpm7jb_4a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS! Good job! \n"
     ]
    }
   ],
   "source": [
    "## Test code  Error 나면 코드 확인필요 \"PASS! Good job\" 이 아무문제 없이 출력시 성공 \n",
    "test_attention_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V38cAqCQ8DGj"
   },
   "source": [
    "**1-2 Attention General Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h3iBPlwE8Osz"
   },
   "outputs": [],
   "source": [
    "class Attention_General(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim = 3):\n",
    "        \"\"\"\n",
    "        :param hidden_dim: feature size of hidden dimension \n",
    "        \"\"\"\n",
    "        super(Attention_General, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "        self._weight_init()\n",
    "    def _weight_init(self):\n",
    "        self.Wa.weight.data = torch.tensor([[1.0,1.0,0.0],[-1.0,1.0,0.0],[0.0,1.0,-1.0]])\n",
    "    def forward(self, key, value, query):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param key: encoded features, a tensor of dimension (batch_size, num_pixels, hidden_state)\n",
    "        :param value: encoded features, a tensor of dimension (batch_size, num_pixels, hidden_state)\n",
    "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, hidden_dim)\n",
    "        :return: \n",
    "          attention weighted encoding : (batch_size, hidden_state) - context vector (attention output )\n",
    "          alpha : (B,num_pixels) -> (attention value, which is normalized by softmax function)\n",
    "        \"\"\"\n",
    "        \n",
    "        # To do ????을 채우세요 \n",
    "        x = self.Wa(key) # query.unsqueeze(1) # ????  # (batch_size, num_pixels, hidden_dim)\n",
    "        \n",
    "        transpose_x_out =  x.permute(0,2,1)\n",
    "        logits = torch.bmm(query.unsqueeze(1), transpose_x_out) # ????# (batch_size,1, hidden_dim)\n",
    "        logits = logits.squeeze(1)\n",
    "        alpha = self.softmax(logits) # ???????  # (batch_size, num_pixels)\n",
    "                \n",
    "        attention_weighted_encoding = torch.bmm(alpha.unsqueeze(1), value) # ?????????  # (batch_size, encoder_dim)\n",
    "\n",
    "        return attention_weighted_encoding, alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OKWZESFfDwlp"
   },
   "source": [
    "#### Test 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9w51W3a5DsU0"
   },
   "outputs": [],
   "source": [
    "#수정하지 말것 \n",
    "def test_attention_general():\n",
    "  model = Attention_General()\n",
    "  test_encoder_out = torch.zeros((3,3,3))\n",
    "  test_decoder_hidden = torch.zeros((3,3))\n",
    "  attention_context, alpha = model(test_encoder_out, test_encoder_out, test_decoder_hidden)\n",
    "  gt_alpha1 = torch.zeros((3,3)).fill_(1/3)\n",
    "  gt_output = torch.zeros((3,3,3))\n",
    "  assert (torch.all(torch.lt(torch.abs(torch.add(alpha, -gt_alpha1)), 1e-4))) == 1\n",
    "  assert (torch.all(torch.lt(torch.abs(torch.add(attention_context, -gt_output)), 1e-4))) == 1\n",
    "  test_encoder_out = torch.tensor([[[1.0,1.0,1.0],[-1.0,-1.0,-1.0],[0.0,0.0,0.0]]])\n",
    "  test_decoder_hidden = torch.tensor([[1.0,1.0,1.0]])\n",
    "  attention_context2, alpha2 = model(test_encoder_out, test_encoder_out, test_decoder_hidden)\n",
    "  gt_alpha2 = torch.tensor([[0.8668, 0.0159, 0.1173]]).float()\n",
    "  gt_context2 = torch.tensor([[0.8509,0.8509,0.8509]]).float()\n",
    "  assert (torch.all(torch.lt(torch.abs(torch.add(alpha2, -gt_alpha2)), 1e-4))) == 1\n",
    "  assert (torch.all(torch.lt(torch.abs(torch.add(attention_context2, -gt_context2)), 1e-4))) == 1\n",
    "  print (\"PASS! Good job! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qoM3JiYSDzZd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS! Good job! \n"
     ]
    }
   ],
   "source": [
    "## Test code  Error 나면 코드 확인필요 \"PASS! Good job\" 이 아무문제 없이 출력시 성공 \n",
    "test_attention_general()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IPkNdCSTI4hj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW_captioning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
